@inproceedings{Messer2022,
   abstract = {Over the last few years, Computer Science class sizes have increased, resulting in a higher grading workload. Universities often use multiple graders to quickly deliver the grades and associated feedback to manage this workload. While using multiple graders enables the required turnaround times to be achieved, it can come at the cost of consistency and feedback quality. Partially automating the process of grading and feedback could help solve these issues. This project will look into methods to assist in grading and feedback partially subjective elements of programming assignments, such as readability, maintainability, and documentation, to increase the marker’s amount of time to write meaningful feedback. We will investigate machine learning and natural language processing methods to improve grade uniformity and feedback quality in these areas. Furthermore, we will investigate how using these tools may allow instructors to include open-ended requirements that challenge students to use their ideas for possible features in their assignments.},
   author = {Marcus Messer},
   doi = {10.1007/978-3-031-11647-6_6},
   issn = {16113349},
   booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   title = {Grading Programming Assignments with an Automated Grading and Feedback Assistant},
   volume = {13356 LNCS},
   year = {2022},
}
@article{Messer2024,
   abstract = {We conducted a systematic literature review on automated grading and feedback tools for programming education. We analysed 121 research papers from 2017 to 2021 inclusive and categorised them based on skills assessed, approach, language paradigm, degree of automation, and evaluation techniques. Most papers assess the correctness of assignments in object-oriented languages. Typically, these tools use a dynamic technique, primarily unit testing, to provide grades and feedback to the students or static analysis techniques to compare a submission with a reference solution or with a set of correct student submissions. However, these techniques’ feedback is often limited to whether the unit tests have passed or failed, the expected and actual output, or how they differ from the reference solution. Furthermore, few tools assess the maintainability, readability, or documentation of the source code, with most using static analysis techniques, such as code quality metrics, in conjunction with grading correctness. Additionally, we found that most tools offered fully automated assessment to allow for near-instantaneous feedback and multiple resubmissions, which can increase student satisfaction and provide them with more opportunities to succeed. In terms of techniques used to evaluate the tools’ performance, most papers primarily use student surveys or compare the automatic assessment tools to grades or feedback provided by human graders. However, because the evaluation dataset is frequently unavailable, it is more difficult to reproduce results and compare tools to a collection of common assignments.},
   author = {Marcus Messer and Neil C.C. Brown and Michael Kölling and Miaojing Shi},
   doi = {10.1145/3636515},
   issn = {19466226},
   issue = {1},
   journal = {ACM Transactions on Computing Education},
   title = {Automated Grading and Feedback Tools for Programming Education: A Systematic Review},
   volume = {24},
   year = {2024},
}
@inproceedings{Krusche2020,
   author = {Stephan Krusche and Nadine von Frankenberg and Lara Marie Reimer and Bernd Bruegge},
   city = {New York, NY, USA},
   doi = {10.1145/3377814.3381701},
   isbn = {9781450371247},
   booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training},
   month = {6},
   pages = {12-22},
   publisher = {ACM},
   title = {An interactive learning method to engage students in modeling},
   year = {2020},
}
@inproceedings{Syaifudin2024,
   author = {Yan Watequlis Syaifudin and Pramana Yoga Saputra and Triana Fatmawati and Nungki Indah Susanti and Abdul Rahman Patta and Aisya Calvina},
   doi = {10.1109/ICETSIS61505.2024.10459673},
   isbn = {979-8-3503-7222-9},
   booktitle = {2024 ASU International Conference in Emerging Technologies for Sustainability and Intelligent Systems (ICETSIS)},
   month = {1},
   pages = {1883-1887},
   publisher = {IEEE},
   title = {Application of Unit Testing and Integration Testing for Automatic Grading Mechanism in Android Programming Learning Assistance System},
   year = {2024},
}
@inproceedings{Iddon2023,
   author = {Callum Iddon and Nasser Giacaman and Valerio Terragni},
   doi = {10.1109/ICSE-SEET58685.2023.00024},
   isbn = {979-8-3503-2259-0},
   booktitle = {2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET)},
   month = {5},
   pages = {192-197},
   publisher = {IEEE},
   title = {GRADESTYLE: GitHub-Integrated and Automated Assessment of Java Code Style},
   year = {2023},
}
@inproceedings{Rajesh2024,
   author = {Sanjay Rajesh and Vineeth V Rao and MG Thushara},
   doi = {10.1109/I2CT61223.2024.10543863},
   isbn = {979-8-3503-9445-0},
   booktitle = {2024 IEEE 9th International Conference for Convergence in Technology (I2CT)},
   month = {4},
   pages = {1-6},
   publisher = {IEEE},
   title = {Comprehensive Investigation of Code Assessment Tools in Programming Courses},
   year = {2024},
}

